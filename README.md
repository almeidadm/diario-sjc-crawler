# DiÃ¡rio Oficial Crawler - SÃ£o JosÃ© dos Campos

![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)
![Packaging: Poetry](https://img.shields.io/badge/packaging-poetry-cyan.svg)
![Async/Await](https://img.shields.io/badge/async-await-green.svg)
![License: MIT](https://img.shields.io/badge/license-MIT-lightgrey.svg)

Um crawler assÃ­ncrono e eficiente para capturar ediÃ§Ãµes e artigos do DiÃ¡rio Oficial de SÃ£o JosÃ© dos Campos. Desenvolvido para processamento em larga escala com armazenamento otimizado em formato Parquet. Gerenciado com Poetry para dependÃªncias consistentes e empacotamento.
ğŸš€ CaracterÃ­sticas Principais

    Crawler AssÃ­ncrono: Processamento concorrente com httpx e asyncio

    Pipeline TrifÃ¡sico: Metadados â†’ Estrutura HTML â†’ ConteÃºdo

    Armazenamento Eficiente: Dados salvos em Parquet com partiÃ§Ã£o temporal

    Processamento em Lotes: Suporte a intervalos longos com checkpoint

    ResiliÃªncia a Falhas: Mecanismos de retry e tratamento de erros robusto

    CLI Completo: Interface de linha de comando para todas as operaÃ§Ãµes

    Gerenciamento com Poetry: DependÃªncias consistentes e ambiente isolado

ğŸ“ Estrutura do Projeto
```text
rag-diario-sjc-crawler/
â”œâ”€â”€ pyproject.toml              # ConfiguraÃ§Ã£o Poetry e dependÃªncias
â”œâ”€â”€ poetry.lock                 # Lock file das dependÃªncias
â”œâ”€â”€ src/diario_crawler/         # CÃ³digo fonte do pacote
â”‚   â”œâ”€â”€ core/                   # OrquestraÃ§Ã£o principal
â”‚   â”œâ”€â”€ http/                   # Cliente HTTP
â”‚   â”œâ”€â”€ parsers/                # Processamento de dados
â”‚   â”œâ”€â”€ processors/             # AgregaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ storage/                # Armazenamento
â”‚   â”œâ”€â”€ utils/                  # UtilitÃ¡rios
â”‚   â””â”€â”€ models/                 # Modelos de dados
â”œâ”€â”€ scripts/                    # Scripts executÃ¡veis
â”œâ”€â”€ notebooks/                  # Jupyter notebooks para anÃ¡lise
â”œâ”€â”€ tests/                      # Testes unitÃ¡rios
â”œâ”€â”€ data/                       # Dados gerados (nÃ£o versionado)
â””â”€â”€ README.md                   # Este arquivo
```

âš¡ InstalaÃ§Ã£o RÃ¡pida
PrÃ©-requisitos

    Python 3.11 ou superior

    Poetry instalado globalmente

InstalaÃ§Ã£o com Poetry

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd rag-diario-sjc-crawler
```

# Instale as dependÃªncias com Poetry
poetry install

# Ative o ambiente virtual
poetry shell

InstalaÃ§Ã£o sem Poetry Shell

```bash
# Execute comandos diretamente sem ativar o shell
poetry run python scripts/run_crawler.py --days 7
```

ğŸ¯ Uso BÃ¡sico
ExecuÃ§Ã£o Simples (Ãšltimos 7 dias)
```bash
poetry run python scripts/run_crawler.py --days 7
```

PerÃ­odo EspecÃ­fico
```bash
poetry run python scripts/run_crawler.py --start-date 2025-01-01 --end-date 2025-01-31
```

Com Logs Detalhados
```bash
poetry run python scripts/run_crawler.py --days 30 --log-level DEBUG --log-file logs/crawler.log
```

ğŸ”§ Uso AvanÃ§ado
Processamento em Lotes (Longo PerÃ­odo)
```bash
poetry run python scripts/batch_process.py \
  --start-date 2025-01-01 \
  --end-date 2025-12-31 \
  --batch-days 15 \
  --max-retries 5
```

Desenvolvimento e ContribuiÃ§Ã£o
```bash
# Instalar dependÃªncias de desenvolvimento
poetry install --with dev

# Executar testes
poetry run pytest

# FormataÃ§Ã£o de cÃ³digo
poetry run black src/ scripts/
poetry run isort src/ scripts/

# VerificaÃ§Ã£o de tipos
poetry run mypy src/
```

ğŸ“¦ DependÃªncias Principais

O projeto utiliza Poetry para gerenciamento de dependÃªncias. Principais pacotes:
Runtime

```toml
httpx = ">=0.24.0"           # Cliente HTTP assÃ­ncrono
pandas = ">=2.0.0"           # ManipulaÃ§Ã£o de dados
pyarrow = ">=12.0.0"         # Formato Parquet
selectolax = ">=0.3.0"       # Parsing HTML rÃ¡pido
python-dateutil = ">=2.8.0"  # UtilitÃ¡rios de data
```

Desenvolvimento
```toml
pytest = ">=7.0.0"           # Testes
black = ">=23.0.0"           # FormataÃ§Ã£o
isort = ">=5.12.0"           # OrganizaÃ§Ã£o de imports
mypy = ">=1.0.0"            # VerificaÃ§Ã£o de tipos
jupyter = ">=1.0.0"          # Notebooks para anÃ¡lise
```

ğŸ—ï¸ Estrutura do CÃ³digo
MÃ³dulos Principais
```python
# Crawler principal
from diario_crawler.core import GazetteCrawler, CrawlerConfig

# Clientes HTTP
from diario_crawler.http import HttpClient, ConcurrentHttpClient

# Parsers
from diario_crawler.parsers import MetadataParser, HtmlStructureParser, ContentParser

# Storage
from diario_crawler.storage import ParquetStorage

# UtilitÃ¡rios
from diario_crawler.utils import get_workdays, setup_logging
```

Exemplo de Uso ProgramÃ¡tico
```python
from datetime import date
from diario_crawler.core import GazetteCrawler, CrawlerConfig
from diario_crawler.storage import ParquetStorage

async def main():
    config = CrawlerConfig(
        start_date=date(2025, 11, 1),
        end_date=date(2025, 11, 5)
    )
    storage = ParquetStorage()
    crawler = GazetteCrawler(config=config, storage=storage)
    
    editions = await crawler.run_and_save()
    print(f"Processadas {len(editions)} ediÃ§Ãµes")

# Execute com asyncio
import asyncio
asyncio.run(main())
```

ğŸ’¾ Armazenamento de Dados
Estrutura com Poetry

Os dados sÃ£o armazenados no diretÃ³rio data/ com a seguinte estrutura:

```text
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ gazettes/            # Metadados das ediÃ§Ãµes
â”‚   â”œâ”€â”€ articles/            # Artigos particionados por data
â”‚   â””â”€â”€ relationships/       # RelaÃ§Ãµes ediÃ§Ã£o-artigo
â””â”€â”€ checkpoints/             # Checkpoints para processamento em lote
```

RecuperaÃ§Ã£o de Dados
```python
from diario_crawler.storage import ParquetStorage

storage = ParquetStorage(base_path="data/raw")

# Carregar ediÃ§Ã£o especÃ­fica com todos os artigos
edition = storage.load_edition_with_articles("2555")

# Carregar todas as ediÃ§Ãµes
all_editions = storage.load_editions()
```

ğŸ“Š Comandos Ãšteis do Poetry

```bash
# Adicionar nova dependÃªncia
poetry add nome-do-pacote

# Adicionar dependÃªncia de desenvolvimento
poetry add --group dev nome-do-pacote

# Atualizar dependÃªncias
poetry update

# Verificar ambiente
poetry env info

# Exportar requirements.txt (se necessÃ¡rio)
poetry export -f requirements.txt --output requirements.txt
```

ğŸš€ Deploy e ProduÃ§Ã£o
Build do Pacote
```bash
# Build do pacote distribuÃ­vel
poetry build

# InstalaÃ§Ã£o em produÃ§Ã£o (sem dependÃªncias de dev)
poetry install --without dev
```

ExecuÃ§Ã£o em ProduÃ§Ã£o

```bash
# Usando o ambiente Poetry
poetry run python scripts/run_crawler.py --days 1

# Ou instalando globalmente
pip install .
python scripts/run_crawler.py --days 1
```

ğŸ› SoluÃ§Ã£o de Problemas
Problemas Comuns

Erro de importaÃ§Ã£o

```bash
# Certifique-se de estar no ambiente Poetry
poetry shell

# Ou use poetry run
poetry run python seu_script.py
```

DependÃªncias faltando

```bash
# Atualize o ambiente
poetry install
```

Logs e Debug

```bash
# Logs detalhados
poetry run python scripts/run_crawler.py --log-level DEBUG

# Log para arquivo
poetry run python scripts/run_crawler.py --log-file crawler.log
```

ğŸ¤ ContribuiÃ§Ã£o

    ConfiguraÃ§Ã£o do Ambiente

```bash
git clone <repo>
cd rag-diario-sjc-crawler
poetry install --with dev
```
    PadrÃµes de CÃ³digo

```bash
# FormataÃ§Ã£o automÃ¡tica
poetry run black src/ scripts/ tests/
poetry run isort src/ scripts/ tests/

# VerificaÃ§Ã£o de tipos
poetry run mypy src/
```
Testes

```bash

poetry run pytest
```

ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo LICENSE para detalhes.
ğŸ†˜ Suporte

Em caso de problemas:

    Verifique se o ambiente Poetry estÃ¡ ativo: poetry env info

    Execute com logs detalhados: --log-level DEBUG

    Consulte as issues abertas no repositÃ³rio

    Crie uma nova issue com detalhes do problema e output do comando poetry env info

Desenvolvido com Poetry para dependÃªncias consistentes e ambiente reproduzÃ­vel ğŸ“¦ğŸ
