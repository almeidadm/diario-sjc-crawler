# DiÃ¡rio Oficial Crawler - SÃ£o JosÃ© dos Campos

![Python Version](https://img.shields.io/badge/python-3.12+-blue.svg)
![Packaging: Poetry](https://img.shields.io/badge/packaging-poetry-cyan.svg)
![Async/Await](https://img.shields.io/badge/async-await-green.svg)
![License: MIT](https://img.shields.io/badge/license-MIT-lightgrey.svg)

Um **crawler assÃ­ncrono e eficiente** para capturar ediÃ§Ãµes e artigos do DiÃ¡rio Oficial de SÃ£o JosÃ© dos Campos.  
Projetado para **processamento em larga escala**, com **armazenamento otimizado em Parquet/DuckDB** e **anÃ¡lises integradas com Polars e Pandas**.

---

## ğŸš€ CaracterÃ­sticas Principais

- **Crawler AssÃ­ncrono** â€” uso de `httpx[http2]` e `asyncio` para alta concorrÃªncia.  
- **Pipeline TrifÃ¡sico** â€” Metadados â†’ Estrutura HTML â†’ ConteÃºdo completo.  
- **Armazenamento Eficiente** â€” suporte a `Parquet` e `DuckDB` com particionamento temporal.  
- **ResiliÃªncia** â€” controle de *retries* e logs enriquecidos via `rich`.  
- **CLI Completo** â€” interface via `scripts/` e integraÃ§Ã£o com `taskipy`.  
- **AnÃ¡lises e VisualizaÃ§Ã£o** â€” integraÃ§Ã£o com `matplotlib`, `seaborn`, `polars` e `pandas`.  
- **Gerenciamento com Poetry** â€” ambientes isolados e consistentes.  

---

## ğŸ“ Estrutura do Projeto

```text
rag-diario-sjc-crawler/
â”œâ”€â”€ pyproject.toml              # ConfiguraÃ§Ã£o Poetry e dependÃªncias
â”œâ”€â”€ poetry.lock                 # Lock file
â”œâ”€â”€ src/diario_crawler/         # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ core/                   # OrquestraÃ§Ã£o principal
â”‚   â”œâ”€â”€ http/                   # Cliente HTTP
â”‚   â”œâ”€â”€ parsers/                # Parsing e extraÃ§Ã£o
â”‚   â”œâ”€â”€ processors/             # Processamento e agregaÃ§Ã£o
â”‚   â”œâ”€â”€ storage/                # Armazenamento (Parquet/DuckDB)
â”‚   â”œâ”€â”€ utils/                  # FunÃ§Ãµes auxiliares e logging
â”‚   â””â”€â”€ models/                 # Estruturas de dados
â”œâ”€â”€ scripts/                    # Scripts executÃ¡veis (CLI)
â”œâ”€â”€ notebooks/                  # Notebooks para exploraÃ§Ã£o e anÃ¡lise
â”œâ”€â”€ tests/                      # Testes unitÃ¡rios (TODO)
â”œâ”€â”€ data/                       # Dados locais (nÃ£o versionados)
â””â”€â”€ README.md
```

---

## âš¡ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python **3.12+**
- Poetry instalado globalmente

### Passos

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd rag-diario-sjc-crawler

# Instale dependÃªncias
poetry install

# Ative o ambiente virtual
poetry shell
```

Ou execute diretamente sem ativar o shell:

```bash
poetry run python scripts/run_crawler.py --days 7
```

---

## ğŸ¯ Uso BÃ¡sico

### Ãšltimos 7 dias

```bash
poetry run python scripts/run_crawler.py --days 7
```

### PerÃ­odo EspecÃ­fico

```bash
poetry run python scripts/run_crawler.py --start-date 2025-01-01 --end-date 2025-01-31
```

### Logs Detalhados

```bash
poetry run python scripts/run_crawler.py --days 30 --log-level DEBUG --log-file logs/crawler.log
```

---

## ğŸ”§ Uso AvanÃ§ado

### Processamento em Lotes

```bash
poetry run python scripts/batch_process.py   --start-date 2025-01-01   --end-date 2025-12-31   --batch-days 15   --max-retries 5
```

### Desenvolvimento

```bash
# DependÃªncias de desenvolvimento
poetry install --with dev

# Lint e formataÃ§Ã£o
task lint
task format
```

---

## ğŸ“¦ DependÃªncias Principais

### Runtime

| Pacote         | DescriÃ§Ã£o                                   |
| -------------- | ------------------------------------------- |
| `httpx[http2]` | Cliente HTTP assÃ­ncrono de alta performance |
| `selectolax`   | Parser HTML rÃ¡pido baseado em lexbor        |
| `rich`         | SaÃ­da de logs e console colorido            |
| `pandas`       | ManipulaÃ§Ã£o tabular                         |
| `pyarrow`      | Suporte ao formato Parquet                  |
| `duckdb`       | Engine analÃ­tica em memÃ³ria e on-disk       |
| `polars`       | DataFrame de alto desempenho                |

### Desenvolvimento (`--with dev`)

| Pacote                  | FunÃ§Ã£o                                  |
| ----------------------- | --------------------------------------- |
| `black`                 | FormataÃ§Ã£o de cÃ³digo                    |
| `isort`                 | OrganizaÃ§Ã£o de imports                  |
| `flake8`                | Linter                                  |
| `taskipy`               | DefiniÃ§Ã£o de tarefas CLI                |
| `matplotlib`, `seaborn` | VisualizaÃ§Ã£o                            |
| `tqdm[notebook]`        | Barra de progresso em terminal/notebook |
| `ipykernel`, `notebook` | Ambiente interativo Jupyter             |

---

## ğŸ§© Tarefas Taskipy

```toml
[tool.taskipy.tasks]
lint = "flake8 src"
format = "isort src && black src"
check = "task format && task lint"
```

Uso:

```bash
task lint    # Verifica cÃ³digo
task format  # Formata cÃ³digo
task check   # Executa lint e formataÃ§Ã£o
```

---

## ğŸ—ï¸ Exemplo ProgramÃ¡tico

```python
from datetime import date
import asyncio
from diario_crawler.core import GazetteCrawler, CrawlerConfig
from diario_crawler.storage import ParquetStorage

async def main():
    config = CrawlerConfig(
        start_date=date(2025, 11, 1),
        end_date=date(2025, 11, 5)
    )
    storage = ParquetStorage()
    crawler = GazetteCrawler(config=config, storage=storage)

    editions = await crawler.run_and_save()
    print(f"Processadas {len(editions)} ediÃ§Ãµes")

asyncio.run(main())
```

---

## ğŸ’¾ Estrutura de Armazenamento

```text
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ gazettes/        # Metadados das ediÃ§Ãµes
â”‚   â”œâ”€â”€ articles/        # Artigos particionados por data
â”‚   â””â”€â”€ relationships/   # RelaÃ§Ãµes ediÃ§Ã£o-artigo
â””â”€â”€ checkpoints/         # Checkpoints de batch
```

### Leitura de Dados

```python
from diario_crawler.storage import ParquetStorage

storage = ParquetStorage(base_path="data/raw")
edition = storage.load_edition_with_articles("2555")
```

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License**.  
Consulte o arquivo `LICENSE` para mais detalhes.
